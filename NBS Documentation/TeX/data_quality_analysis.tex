\section{Data Quality Analysis}

During the exploration of the NBS system, a comprehensive analysis of the stored data was conducted. This section outlines the major data quality issues identified, including data incompleteness, inconsistencies, and duplicates. These issues significantly affect the systemâ€™s reliability, making data analysis both difficult and time-consuming.

\subsection{Data Collection and Methodology}

Data was collected using both the Dataverse Web API and the Power BI Web API. A Python script, \texttt{api\_connect.py}, was developed, which includes a \texttt{DataverseConnector} class and a \texttt{connect\_to\_powerbi} function. The script manages API connections and user authentication. More detailed information on the script can be found in the \hyperref[sec:Appendices]{Appendices} and \hyperref[sec:CodeDocumentation]{Code Documentation}. The code itself is also thoroughly documented with Docstrings and comments.

Data from each table was saved as \texttt{.csv} files, available in the \hyperref[sec:Appendices]{Appendices}. These files serve as the primary dataset for the analysis in this document.

Following data collection, the exploration and analysis of NBS began. Using data extracted from the API, the structure of the entities and their relationships were explored and visualized using an ER Diagram. This provided insights into which tables were essential, which served as dictionaries, and which contained observational data.

Subsequently, the collected data was analyzed for quality and consistency, focusing on missing values, duplicated records, and formatting issues. Data from multiple tables was aggregated into larger datasets to provide a more comprehensive view. Each table is thoroughly documented.

The analysis was primarily conducted using Python, utilizing libraries such as pandas, numpy, matplotlib, and seaborn. Basic statistical methods were employed to gain a deeper understanding of the data.

\subsection{Data Quality Issues}

The overall quality of the dataset is poor, making data cleaning and preparation for analysis tedious and time-consuming. This section provides an overview of the common issues encountered during the exploration and analysis of the NBS data.

\subsubsection{Nondescriptive Logical Column Names}
Many columns have unclear or incorrect logical names, making it difficult to understand their purpose. A common issue is columns ending in \texttt{...id} that store names instead of unique IDs, as expected.

\textbf{Example}\\
The \texttt{cr675\_testorder} table has a column \texttt{cr91b\_depthofpenetrationinmm}, which stores boolean values (\texttt{True}/\texttt{False}) instead of numeric data. Upon investigation, this column was found to indicate whether the test order was for hard or soft ballistics. A more appropriate name would be \texttt{cr91b\_ishardballistics}.

\newpage

\subsubsection{Inconsistent Column Formats}
Most categorical columns suffer from inconsistent formats, making analysis difficult or impossible.

\textbf{Example}\\
The \texttt{cr675\_testdashboardammotypedictionary} table contains a \texttt{cr675\_bulletcalliber} column, which is of \texttt{string} type. Example values include:

\begin{table}[h!]
	\centering
	\begin{tabular}{|l|}
	\hline
	\textbf{BulletCaliber} \\
	\hline
	9 x 19 mm \\
	5.56 \\
	2,642mm \\
	7,62 \\
	7,62x39 AK47 \\
	308 Win. \\
	.40 / 10 mm \\
	.357 in \\
	\hline
	\end{tabular}
\end{table}

This data is not useful for analysis without extensive cleaning and standardization. Different formats, additional information stored alongside the caliber (e.g., \texttt{AK47}), ambiguities in values and units make it very hard to analyze.

For data to be useful and ready for analysis, it needs to be standardized and consistent. The \texttt{cr675\_bulletcalliber} should store a caliber value in one unit of choice (e.g., millimeters or inches) with consistent formatting. Using the above example, the proper data should look like this:

\begin{table}[h!]
	\centering
	\begin{tabular}{|l|}
	\hline
	\textbf{BulletCaliber} \\
	\hline
	9.00 \\
	5.56 \\
	2.64 \\
	7.62 \\
	7.62 \\
	7.82 \\
	10.00 \\
	9.00 \\
	\hline
	\end{tabular}
\end{table}

\subsubsection{Wrong Column Types}
Many columns are stored as strings when they should contain uniform numeric values, further complicating analysis.

\newpage

\subsubsection{Duplicate Information}
Several columns contain redundant information. In some cases, multiple columns needed to be merged to obtain meaningful data during the exploration process.

\textbf{Example}\\
Using the same table as above, it contains columns \texttt{cr675\_produceraccount}, \texttt{cr675\_name}, and \texttt{cr675\_bulletnamewithproducername}. The last one is just a join of the two previous columns and is redundant. Moreover, most of the \texttt{cr675\_produceraccount} column is empty, so \texttt{cr675\_bulletnamewithproducername} contains just a duplicate of \texttt{cr675\_name} with a colon appended.

\subsubsection{High Percentage of Empty Columns}
A significant portion of the columns are largely empty, with minimal data recorded. This indicates a lack of enforcement of data entry standards and contributes to the overall data incompleteness.

\textbf{Example}\\
In the \texttt{cr675\_testdashboardprocedement} etable, 13 custom columns have less than 50\% data completeness, with six of these columns entirely empty.

\subsubsection{Lack of Required Fields}
Few columns are marked as "required," resulting in significant data gaps. This exacerbates the incompleteness of the dataset and reduces its overall reliability.

\subsubsection{Redundant Tables and Columns}
Some tables and columns appear to be redundant, either due to duplication or subsetting of data from other tables. This redundancy adds unnecessary complexity to the data model.

\subsection{Summary}
The data is largely unusable, even for cleaning. Poor data input enforcement has resulted in incomplete datasets, and the lack of formatting and column type standardization has left most data stored as text, often with redundant or unstructured information, complicating analysis.

These tables appear to be used by engineers as if they were Excel spreadsheets, rather than being treated as structured datasets. Collecting data in this manner significantly hampers analysis, as it often requires laborious data cleaning, including verifying individual rows, cross-referencing documentation, or consulting domain experts.